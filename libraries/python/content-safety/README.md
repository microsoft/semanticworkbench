# Content Safety Evaluators for Semantic Workbench Assistants

Use these evaluators to ensure that the content being processed by your assistant and being displayed to users is safe and appropriate. This is especially important when dealing with user-generated or model-generated content, but can also be useful for code-generated content as well.

See the [Responsible AI FAQ](../../RESPONSIBLE_AI_FAQ.md) for more information.

## Recommended

The recommended evaluator is the [`combined-content-safety-evaluator`](./combined-content-safety-evaluator/README.md) which is a complete package that includes all of the evaluators in this repository. Alternatively, you can use the individual evaluators if you only need one or two of them. See the README files in each evaluator's directory for more information.
