{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Driver\n",
    "\n",
    "An OpenAI Chat Completions API wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "Run this cell to set the notebook up. Other sections can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import aio, DefaultAzureCredential, get_bearer_token_provider, AzureCliCredential\n",
    "\n",
    "from openai import AsyncAzureOpenAI, AzureOpenAI\n",
    "\n",
    "import logging \n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOGGING = {\n",
    "    \"version\": 1,\n",
    "    \"disable_existing_loggers\": False,\n",
    "    \"formatters\": {\n",
    "        \"json\": {\n",
    "            \"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n",
    "            \"fmt\": \"%(asctime)s %(levelname)s %(name)s %(message)s\",\n",
    "\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Set up structured logging to a file. All of the cells in this notebook use\n",
    "# this logger. Find them at .data/logs.jsonl.\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record) -> str:\n",
    "        record_dict = record.__dict__\n",
    "        log_record = {\n",
    "            'timestamp': self.formatTime(record, self.datefmt),\n",
    "            'level': record.levelname,\n",
    "            'session_id': record_dict.get('session_id', None),\n",
    "            'run_id': record_dict.get('run_id', None),\n",
    "            'message': record.getMessage(),\n",
    "            'data': record_dict.get('data', None),\n",
    "            'module': record.module,\n",
    "            'funcName': record.funcName,\n",
    "            'lineNumber': record.lineno,\n",
    "            'logger': record.name,\n",
    "        }\n",
    "        extra_fields = {\n",
    "            key: value for key, value in record.__dict__.items() \n",
    "            if key not in ['levelname', 'msg', 'args', 'exc_info', 'funcName', 'module', 'lineno', 'name', 'message', 'asctime', 'session_id', 'run_id', 'data']\n",
    "        }\n",
    "        log_record.update(extra_fields)\n",
    "        return json.dumps(log_record)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "modules = ['httpcore.connection', 'httpcore.http11', 'httpcore.sync.connection', 'httpx', 'openai', 'urllib3.connectionpool', 'urllib3.util.retry']\n",
    "for module in modules:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "data_dir = Path('.data')\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "handler = logging.FileHandler(data_dir / 'logs.jsonl')\n",
    "handler.setFormatter(JsonFormatter())\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_openai_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "    \"max_retries\": 2,\n",
    "}\n",
    "logger.info(\"Azure OpenAI configuration\", extra=azure_openai_config)\n",
    "\n",
    "async_client = AsyncAzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=aio.get_bearer_token_provider(\n",
    "        aio.AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=get_bearer_token_provider(\n",
    "        AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "model: str = azure_openai_config.get(\"azure_deployment\", \"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatCompletionsAPI usage\n",
    "\n",
    "Azure/OpenAI's Chat Completions API is the fundamental building block of an AI assistant that uses the GPT model. \n",
    "\n",
    "- https://platform.openai.com/docs/api-reference/chat\n",
    "- https://github.com/openai/openai-python/blob/main/api.md\n",
    "- https://platform.openai.com/docs/api-reference/chat drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-AWSTEn1cZqu47WD3TU0R1cTQQjWCQ\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"This is a test.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1732299300,\n",
      "  \"model\": \"gpt-4o-2024-08-06\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": \"fp_d54531d9eb\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 5,\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 17,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-ASsXGnCRUzKrj4CWU1Zbc1ZXaTRqm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1731446182, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_d54531d9eb', usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "message_event = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(message_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'async_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43masync_client\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m         {\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay this is a test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         }\n\u001b[1;32m      7\u001b[0m     ],\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      9\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mmodel_dump())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'async_client' is not defined"
     ]
    }
   ],
   "source": [
    "stream = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    stream=True,\n",
    ")\n",
    "async for chunk in stream:\n",
    "    print(chunk.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized response handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is vast and holds immense potential to transform nearly every aspect of our lives. As we look forward, here are several key domains where AI is likely to make significant impacts:\n",
      "\n",
      "1. **Healthcare**: AI will continue to revolutionize healthcare through personalized medicine, improved diagnostic capabilities, and efficient drug discovery. Machine learning algorithms can analyze patient data to suggest tailored treatment plans, detect diseases earlier, and even manage administrative tasks.\n",
      "\n",
      "2. **Transportation**: Autonomous vehicles and AI-driven logistics will reshape transportation infrastructure and mobility. This includes not only self-driving cars but also the optimization of public transport systems and delivery services.\n",
      "\n",
      "3. **Environment**: AI will play a crucial role in addressing environmental challenges. From optimizing energy consumption to predicting and mitigating natural disasters, AI can help manage resources more sustainably and minimize human impact on the planet.\n",
      "\n",
      "4. **Workplace Automation**: While AI will automate routine tasks, it will also create opportunities for innovation and new job categories. We need to focus on reskilling and upskilling the workforce to harness AI as a tool for enhancing productivity and creativity.\n",
      "\n",
      "5. **Ethics and Governance**: As AI systems become more integrated into our lives, ethical considerations and robust governance frameworks will become increasingly important. We must ensure that AI is developed and deployed responsibly, with fairness, transparency, and accountability at the forefront.\n",
      "\n",
      "6. **Education**: AI-powered tools can provide personalized learning experiences, helping educators better meet the diverse needs of students and enabling lifelong learning.\n",
      "\n",
      "Ultimately, the future of AI depends on how we choose to guide its development. If approached thoughtfully and inclusively, it has the potential to enhance human capabilities, address global challenges, and improve quality of life across the globe. It's crucial that as we advance these technologies, we remain vigilant about their societal implications, striving for a future that benefits all.\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.logging import make_completion_args_serializable, add_serializable_data\n",
    "from openai_client.completion import message_content_from_completion\n",
    "\n",
    "context = Context(\"conversation-id-1005\")\n",
    "\n",
    "# We use a metadata dictionary in our helpers to store information about the\n",
    "# completion request.\n",
    "metadata = {}\n",
    "\n",
    "# This is just standard OpenAI completion request arguments.\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and answer thoughtfully.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# If we these completion args to logs and metadata, though, they need to be\n",
    "# serializable. We have a helper for that.\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "\n",
    "# We have helpers for validating the response and handling exceptions in a\n",
    "# standardized way. These ensure that logging happens and metadata is loaded up\n",
    "# properly.\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "\n",
    "    # This helper looks for any error-like situations (the model refuses to\n",
    "    # answer, content filters, incomplete responses) and throws exceptions that\n",
    "    # are handled by the next helper. The first argument is an identifier that\n",
    "    # will be used for logs and metadata namespacing.\n",
    "    validate_completion(completion)\n",
    "    logger.debug(\"completion response.\", extra=add_serializable_data(completion))\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "\n",
    "except Exception as e:\n",
    "    # This helper processes all the types of error conditions you might get from\n",
    "    # the OpenAI API in a standardized way.\n",
    "    completion_error = CompletionError(e)\n",
    "    print(completion_error)\n",
    "    print(completion_error.body)\n",
    "\n",
    "else:\n",
    "    # The message_string helper is used to extract the response from the completion\n",
    "    # (which can get tedious).\n",
    "    print(message_content_from_completion(completion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"AI has been advancing rapidly, impacting various industries and society as a whole. The future will likely see even more profound changes as AI technologies continue to evolve.\",\n",
      "  \"answer\": \"The future of AI holds numerous possibilities, such as enhanced automation, improved decision-making processes, and personalization across various sectors like healthcare, finance, and transportation. We may also witness the development of more sophisticated natural language processing and AI systems that can better understand and act upon human emotions. However, this future will also require addressing ethical and privacy concerns, as well as rethinking workforce dynamics due to potential job displacement.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "\n",
    "from openai_client.logging import make_completion_args_serializable, add_serializable_data\n",
    "from openai_client.completion import message_content_dict_from_completion, JSON_OBJECT_RESPONSE_FORMAT\n",
    "\n",
    "context = Context(\"conversation-id-1002\")\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and return your answer as valid JSON like { \\\"thoughts\\\": <some thoughts>, \\\"answer\\\": <an answer> }.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": JSON_OBJECT_RESPONSE_FORMAT,\n",
    "}\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "    validate_completion(completion)\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    logger.error(completion_error.message, extra=add_serializable_data({\"error\": completion_error.body, \"metadata\": metadata}))\n",
    "else:\n",
    "    message = message_content_dict_from_completion(completion)\n",
    "    print(json.dumps(message, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured output\n",
    "\n",
    "Any Pydantic BaseModel can be used as the \"response_format\" and OpenAI will try to load it up for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"As I consider the possibilities, I see AI evolving to become more seamlessly integrated into our daily lives, potentially transforming industries like healthcare, transportation, and education in significant ways.\",\n",
      "  \"answer\": \"The future of AI promises to bring advancements that will help us solve complex problems and push the boundaries of what is currently possible. We can expect AI to become more adaptive, explainable, and ethical, with systems that not only increase efficiency and productivity but also enhance human capabilities and decision-making processes. As we move forward, it will be crucial to guide AI's development with robust ethical standards and regulations to ensure it benefits society as a whole. The potential for AI to improve personalized medicine, autonomous vehicles, and environmental sustainability, among other areas, is vast—all while maintaining a focus on preserving human values and privacy.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from pydantic import BaseModel\n",
    "from typing import cast\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.logging import add_serializable_data, make_completion_args_serializable\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    thoughts: str\n",
    "    answer: str\n",
    "\n",
    "context = Context(\"conversation-id-1002\")\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and return your thoughtful answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": Output,\n",
    "}\n",
    "\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "    validate_completion(completion)\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    logger.error(completion_error.message, extra=add_serializable_data({\"error\": completion_error.body, \"metadata\": metadata}))\n",
    "else:\n",
    "    # The parsed message is in the `parsed` attribute.\n",
    "    output = cast(Output, completion.choices[0].message.parsed)\n",
    "    print(output.model_dump_json(indent=2))\n",
    "\n",
    "    # Or you can just get the text of the message like usual.\n",
    "    # print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple tool usage\n",
    "\n",
    "The OpenAI chat completions API used the idea of \"tools\" to let the model request running a local tool and then processing the output. To use it, you need to create a JSON Schema representation of the function you want to use as a tool, check the response for the model requesting to run that function, run the function, and give the model the results of the function run for a final call.\n",
    "\n",
    "While you can continue doing all of this yourself, our `complete_with_tool_calls` helper function makes this all easier for you.\n",
    "\n",
    "Instead of generating JSON schema and executing functions yourself, you can use our `ToolFunctions` class to define the functions you want to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square of 53 is 2809.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "def square_the_number(number: int) -> int:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    return number * number\n",
    "\n",
    "\n",
    "tool_functions = ToolFunctions([\n",
    "    ToolFunction(square_the_number),\n",
    "])\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the square of 53?\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, tool_functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        print(completion.choices[0].message.content)\n",
    "        # print(json.dumps(metadata, indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured tool inputs and output\n",
    "\n",
    "You can use Pydantic models as input to tool function arguments.\n",
    "\n",
    "You can also tell the model to respond with structured JSON or Pydantic model structures.\n",
    "\n",
    "Here's an example of doing both things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"I used a function to obtain current weather data for the specified ZIP code, which is 90210 (Beverly Hills, CA). The weather is typically warm and sunny during this season.\",\n",
      "  \"answer\": \"The current weather in 90210 (Beverly Hills, CA) is sunny with a temperature of 25°C (77°F), and the cloud cover is minimal at 20%. Perfect for outdoor activities!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import cast\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    zipcode: str\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    description: str = Field(description=\"The weather description.\")\n",
    "    cloud_cover: float\n",
    "    temp_c: float\n",
    "    temp_f: float\n",
    "\n",
    "def get_weather(input: Input) -> Weather:\n",
    "    \"\"\"Return the weather.\"\"\"\n",
    "    return Weather(description=\"Sunny\", cloud_cover=0.2, temp_c=25.0, temp_f=77.0)\n",
    "\n",
    "class Output(BaseModel):\n",
    "    thoughts: str\n",
    "    answer: str\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is the weather in 90210?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": Output,\n",
    "}\n",
    "\n",
    "functions = ToolFunctions([\n",
    "    ToolFunction(get_weather),\n",
    "])\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        # The parsed message is in the `parsed` attribute.\n",
    "        output = cast(Output, completion.choices[0].message.parsed)\n",
    "        print(output.model_dump_json(indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n",
    "\n",
    "    # Or you can just get the text of the message like usual.\n",
    "    # print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool functions with shadowed locals\n",
    "\n",
    "If you want to make a local function available to be run as a chat completion\n",
    "tool, you can. However, oftentimes, the local function might have some extra\n",
    "arguments that you don't want the model to have to fill out for you in a tool\n",
    "call. In this case, you can create a wrapper function that has the same\n",
    "signature as the tool call and then calls the local function with the extra\n",
    "arguments filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish calculation of 53 results in the binary number `0b101011111001`. If you need any further analysis or conversion, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "# Here is the real function that does the work.\n",
    "def real_square_the_number(number: int, binary: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        return bin(number * number)\n",
    "    return str(number * number)\n",
    "\n",
    "\n",
    "# Here is the wrapper function that whose signature will be used as the tool\n",
    "# call. You can just have it calls the real function with the extra arguments\n",
    "# filled in.\n",
    "def fish_calc(number: int) -> str:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    return real_square_the_number(number, binary=True)\n",
    "\n",
    "# Add then just add wrapper to the tool functions you pass to the\n",
    "# `complete_with_tool_calls` function. This is a way you can expose _any_\n",
    "# function to be called by the model, but with the args you want the model to\n",
    "# fill in!\n",
    "tool_functions = ToolFunctions([\n",
    "    ToolFunction(fish_calc),\n",
    "])\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Run a fish calculation on 53 for me.\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, tool_functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        print(completion.choices[0].message.content)\n",
    "        # print(json.dumps(metadata, indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion Driver (a.k.a \"chat driver\")\n",
    "\n",
    "### OpenAI Assistants\n",
    "\n",
    "The Azure/OpenAI Assistants API is newer, stateful API that splits an `assistant` from the data about a conversation `thread` that can be `run` against an `assistant`. Additionally, you can add `tools` to an assistant that enable the assistant to have more interactive capabilities. The tools currently available are:\n",
    "\n",
    "- *Functions*: Registering local functions with the assistant so it knows it can call them before generating a response. This is a \"hold on let me look that up for you\" kind of interaction.\n",
    "- *File Search* (formerly the retrieval plugin): Attach one or more files and they will be RAG-vectorized and available as content to the assistant.\n",
    "- *Code Interpreter*: Run python code in a secure sandbox.\n",
    "\n",
    "The Assistant API productized as OpenAI's `GPTs` product. The `GPT Builder` lets developers create and deploy GPTs assistants using a web interface.\n",
    "\n",
    "### Chat Driver\n",
    "\n",
    "But an \"assistant\" requires pretty strong \"abstraction lock-in\". This thing isn't really an assistant in the fullest sense... it's more like a \"pseudo-assistant\", but this confuses things. Let's just let the Chat Completion API be what it is and drive it as necessary as we create our assistants. Let's just wrap up the function calling bits (which, ultimately, can give you the other tools like Functions and File Search) in a simple-to-use GPT-like interface we'll call a *chat driver*.\n",
    "\n",
    "The chat driver is meant to be used the exact way the Chat Completions API is... just easier.\n",
    "\n",
    "Our chat driver provides:\n",
    "\n",
    "- The ability to almost magically register functions to the function tool.\n",
    "- Tracking of message history using in-memory, local, or custom message providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the simplest usage of a chat driver\n",
    "\n",
    "Notice that a .data directory is created by default. This is where the conversation history is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is incredibly exciting and holds the potential to fundamentally transform various aspects of our lives, industries, and societies. First and foremost, we're seeing AI become increasingly integrated into our daily routines, from virtual assistants that help manage our schedules to smart home devices that optimize our living environments.\n",
      "\n",
      "In healthcare, AI will continue to revolutionize the industry with advancements in personalized medicine, predictive analytics, and automated diagnostics, leading to more efficient and accurate treatment options. In areas like finance, AI can enhance decision-making through better risk assessment and fraud detection.\n",
      "\n",
      "The future of AI also includes significant advancements in natural language processing and computer vision, which will drive innovations in autonomous vehicles, intelligent virtual agents, and other fields requiring human-like perception and interaction.\n",
      "\n",
      "Moreover, AI will play a crucial role in addressing global challenges such as climate change, by analyzing vast amounts of environmental data to develop more sustainable practices and technologies. In education, AI can offer personalized learning experiences tailored to the needs of each student, enhancing the accessibility and quality of education globally.\n",
      "\n",
      "However, along with these prospects come important ethical considerations surrounding privacy, data security, and algorithmic bias. It is essential for us as a community to ensure that AI is developed and deployed responsibly, with a focus on transparency and inclusivity.\n",
      "\n",
      "Finally, as AI continues to evolve, it could also lead to significant changes in the workforce, reshaping job markets and requiring us to rethink how education and skill development align with future needs.\n",
      "\n",
      "Ultimately, the future of AI is likely to be characterized by a delicate balance between innovation and ethical stewardship, necessitating collaboration across disciplines, industries, and borders to maximize benefits while minimizing potential risks.\n"
     ]
    }
   ],
   "source": [
    "from openai_client.chat_driver import ChatDriver, ChatDriverConfig\n",
    "\n",
    "instructions = \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members.\"\n",
    "\n",
    "chat_driver = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "    ),\n",
    ")\n",
    "\n",
    "message_event = await chat_driver.respond(\"What is the future of AI?\")\n",
    "print(message_event.message)\n",
    "# print(message_event.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can register functions to chat drivers\n",
    "\n",
    "Chat drivers will use any functions you give it as both OpenAI tool calls, and as commands.\n",
    "\n",
    "With each response call, you can specify what type of response you want to have... string, dictionary, or Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello Paul! How can I assist you today?\n",
      "\n",
      "Commands:\n",
      "echo(text: str): Return the text.\n",
      "erase(name: str): Erases a stored value.\n",
      "get_file_contents(file_path: str): Return the contents of a file.\n",
      "\n",
      "Args:\n",
      "- file_path: The path to the file.\n",
      "get_weather(input: Input): Return the weather.\n",
      "help(): Return this help message.\n",
      "json_thing(): Return json.\n",
      "\n",
      "Echoing: Echo this.\n",
      "\n",
      "The contents of \"123.txt\" are: \"The purpose of life is to be happy.\" How else can I help you?\n",
      "\n",
      "{\"description\":\"Sunny\",\"cloud_cover\":0.2,\"temp_c\":25.0,\"temp_f\":77.0}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, cast\n",
    "from openai_client.chat_driver import ChatDriver, ChatDriverConfig\n",
    "from openai_client.chat_driver import LocalMessageHistoryProvider\n",
    "from pydantic import BaseModel, Field\n",
    "from openai_client.tools import ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "# When an chat driver is created, it will automatically create a context with a\n",
    "# session_id. Or, if you want to use a specific session_id, you can pass it as\n",
    "# an argument. This is useful for scoping this chat driver instance to an\n",
    "# external identifier.\n",
    "session_id = \"conversation-id-1002\"\n",
    "\n",
    "\n",
    "# Define tool functions for the chat driver.\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the contents of a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path: The path to the file.\n",
    "    \"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{context.session_id}: {name} erased\"\n",
    "\n",
    "def json_thing() -> dict[str, Any]:\n",
    "    \"\"\"Return json.\"\"\"\n",
    "    return {\"key\": \"value\"}\n",
    "\n",
    "class Input(BaseModel):\n",
    "    zipcode: str\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    description: str = Field(description=\"The weather description.\")\n",
    "    cloud_cover: float\n",
    "    temp_c: float\n",
    "    temp_f: float\n",
    "\n",
    "def get_weather(input: Input) -> Weather:\n",
    "    \"\"\"Return the weather.\"\"\"\n",
    "    return Weather(description=\"Sunny\", cloud_cover=0.2, temp_c=25.0, temp_f=77.0)\n",
    "\n",
    "# Define the chat driver.\n",
    "instructions = \"You are a helpful assistant.\"\n",
    "\n",
    "all_funcs = [ get_file_contents, erase, json_thing, get_weather ]\n",
    "\n",
    "chat_driver = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        # message_provider=message_provider,\n",
    "        commands=all_funcs,  # Commands can be registered when instantiating the chat driver.\n",
    "        functions=all_funcs,  # Functions can be registered when instantiating the chat driver.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Let's clear the data from previous runs by using a custom message provider.\n",
    "message_provider = cast(LocalMessageHistoryProvider, chat_driver.message_provider)\n",
    "message_provider.delete_all()\n",
    "\n",
    "\n",
    "# You can also use the `register_function` decorator to register a function.\n",
    "@chat_driver.register_function_and_command\n",
    "def echo(text: str) -> str:\n",
    "    \"\"\"Return the text.\"\"\"\n",
    "    return f\"Echoing: {text}\"\n",
    "\n",
    "\n",
    "# You can also register functions manually.\n",
    "chat_driver.register_function_and_command(get_file_contents)\n",
    "\n",
    "# Let's see if the agent can respond.\n",
    "message_event = await chat_driver.respond(\"Hi, my name is Paul.\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Help command (shows command available).\n",
    "message_event = await chat_driver.respond(\"/help\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# We can run any function or command directly.\n",
    "message_event = await chat_driver.functions.echo(\"Echo this.\")\n",
    "print()\n",
    "print(message_event)\n",
    "\n",
    "# Let's see if the chat driver has the ability to run it's own registered function.\n",
    "message_event = await chat_driver.respond(\"Please tell me what's in file 123.txt.\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Stuctured output.\n",
    "message_event = await chat_driver.respond(\"What is the weather in 90210?\", response_format=Weather)\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Let's see the full response event.\n",
    "# print()\n",
    "# print(response.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with a chat driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi!\n",
      "Assistant: Hello! How can I assist you today?\n",
      "User: What's the capital of America?\n",
      "Assistant: The capital of the United States of America is Washington, D.C.\n",
      "User: No, I meant South America.\n",
      "Assistant: South America is a continent composed of multiple countries, each with its own capital. Could you specify which country's capital you're interested in within South America?\n",
      "User: Mexico.\n",
      "Assistant: Mexico is actually part of North America. The capital of Mexico is Mexico City.\n",
      "User: Brazil.\n",
      "Assistant: The capital of Brazil is Brasília.\n"
     ]
    }
   ],
   "source": [
    "from openai_client.chat_driver import ChatDriverConfig, ChatDriver\n",
    "from context import Context\n",
    "from openai_client.tools import ToolFunction, ToolFunctions\n",
    "\n",
    "\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Returns the contents of a file.\"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{session_id}: {name} erased\"\n",
    "\n",
    "\n",
    "def echo(value: str) -> str:  # noqa: F811\n",
    "    \"\"\"Echos a value as a string.\"\"\"\n",
    "    match value:\n",
    "        case str():\n",
    "            return value\n",
    "        case list():\n",
    "            return \", \".join(map(str, value))\n",
    "        case dict():\n",
    "            return json.dumps(value)\n",
    "        case int() | bool() | float():\n",
    "            return str(value)\n",
    "        case _:\n",
    "            return str(value)\n",
    "\n",
    "# Define the chat driver.\n",
    "chat_driver_config = ChatDriverConfig(\n",
    "    openai_client=async_client,\n",
    "    model=model,\n",
    "    instructions=\"You are an assistant that has access to a sand-boxed Posix shell.\",\n",
    "    commands=[ get_file_contents, erase, echo ],\n",
    "    functions=[ get_file_contents, erase, echo ],\n",
    ")\n",
    "\n",
    "chat_driver = ChatDriver(chat_driver_config)\n",
    "\n",
    "# Note: Look in the .data directory for the logs, message history, and other data.\n",
    "\n",
    "# Chat with the skill.\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    message_event = await chat_driver.respond(message)\n",
    "    if message_event.metadata.get(\"error\"):\n",
    "        print(f\"Error: {message_event.metadata.get('error')}\")\n",
    "        print(message_event.to_json())\n",
    "        continue\n",
    "    # You can print the entire message event! \n",
    "    # print(response.to_json())\n",
    "    print(f\"Assistant: {message_event.message}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Driver with an Assistant Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from typing import Any, BinaryIO\n",
    "from openai_client.chat_driver import ChatDriverConfig, ChatDriver, ChatDriverConfig\n",
    "from context import Context\n",
    "from assistant_drive import Drive, DriveConfig, IfDriveFileExistsBehavior \n",
    "\n",
    "def get_drive_from_context(context):\n",
    "    return Drive(DriveConfig(root=f\".data/drive/{context.session_id}\"))\n",
    "\n",
    "def write_file_contents(file_path: str, contents: str) -> str:\n",
    "    \"\"\"Writes the contents to a file.\"\"\"\n",
    "    drive = get_drive_from_context(context)\n",
    "    content_bytes: BinaryIO = BytesIO(contents.encode(\"utf-8\"))\n",
    "    drive.write(content_bytes, file_path, if_exists=IfDriveFileExistsBehavior.OVERWRITE)\n",
    "    return f\"{file_path} updated.\"\n",
    "\n",
    "def read_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Returns the contents of a file.\"\"\"\n",
    "    drive = get_drive_from_context(context)\n",
    "    with drive.open_file(file_path) as file:\n",
    "        return file.read().decode(\"utf-8\")\n",
    "\n",
    "functions = [write_file_contents, read_file_contents]\n",
    "\n",
    "# Define the chat driver.\n",
    "chat_driver_config = ChatDriverConfig(\n",
    "    openai_client=async_client,\n",
    "    model=model,\n",
    "    instructions=\"You are an assistant that has access to a sand-boxed Posix shell.\",\n",
    "    commands=functions,\n",
    "    functions=functions,\n",
    ")\n",
    "\n",
    "chat_driver = ChatDriver(chat_driver_config)\n",
    "\n",
    "# Note: Look in the .data directory for the logs, message history, and other data.\n",
    "\n",
    "# Chat with the skill.\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    message_event = await chat_driver.respond(message)\n",
    "    # You can print the entire response event! \n",
    "    # print(response.to_json())\n",
    "    print(f\"Assistant: {message_event.message}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
