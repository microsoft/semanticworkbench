{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Driver\n",
    "\n",
    "An OpenAI Chat Completions API wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "Run this cell to set the notebook up. Other sections can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import aio, DefaultAzureCredential, get_bearer_token_provider, AzureCliCredential\n",
    "\n",
    "from openai import AsyncAzureOpenAI, AzureOpenAI\n",
    "\n",
    "import logging \n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up structured logging to a file.\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record) -> str:\n",
    "        record_dict = record.__dict__\n",
    "        log_record = {\n",
    "            'timestamp': self.formatTime(record, self.datefmt),\n",
    "            'level': record.levelname,\n",
    "            'session_id': record_dict.get('session_id', None),\n",
    "            'run_id': record_dict.get('run_id', None),\n",
    "            'message': record.getMessage(),\n",
    "            'data': record_dict.get('data', None),\n",
    "            'module': record.module,\n",
    "            'funcName': record.funcName,\n",
    "            'lineNumber': record.lineno,\n",
    "            'logger': record.name,\n",
    "        }\n",
    "        extra_fields = {\n",
    "            key: value for key, value in record.__dict__.items() \n",
    "            if key not in ['levelname', 'msg', 'args', 'exc_info', 'funcName', 'module', 'lineno', 'name', 'message', 'asctime', 'session_id', 'run_id', 'data']\n",
    "        }\n",
    "        log_record.update(extra_fields)\n",
    "        return json.dumps(log_record)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "modules = ['httpcore.connection', 'httpcore.http11', 'httpcore.sync.connection', 'httpx', 'openai', 'urllib3.connectionpool', 'urllib3.util.retry']\n",
    "for module in modules:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "data_dir = Path('.data')\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "handler = logging.FileHandler(data_dir / 'logs.jsonl')\n",
    "handler.setFormatter(JsonFormatter())\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_openai_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "    \"max_retries\": 2,\n",
    "}\n",
    "\n",
    "model = azure_openai_config.get(\"azure_deployment\", \"gpt-4o\")\n",
    "\n",
    "async_client = AsyncAzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=aio.get_bearer_token_provider(\n",
    "        aio.AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=get_bearer_token_provider(\n",
    "        AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatCompletionsAPI usage\n",
    "\n",
    "Azure/OpenAI's Chat Completions API is the fundamental building block of an AI assistant that uses the GPT model. \n",
    "\n",
    "- https://platform.openai.com/docs/api-reference/chat\n",
    "- https://github.com/openai/openai-python/blob/main/api.md\n",
    "- https://platform.openai.com/docs/api-reference/chat drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    stream=True,\n",
    ")\n",
    "async for chunk in stream:\n",
    "    print(chunk.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion Driver (a.k.a \"chat driver\")\n",
    "\n",
    "### OpenAI Assistants\n",
    "\n",
    "The Azure/OpenAI Assistants API is newer, stateful API that splits an `assistant` from the data about a conversation `thread` that can be `run` against an `assistant`. Additionally, you can add `tools` to an assistant that enable the assistant to have more interactive capabilities. The tools currently available are:\n",
    "\n",
    "- *Functions*: Registering local functions with the assistant so it knows it can call them before generating a response. This is a \"hold on let me look that up for you\" kind of interaction.\n",
    "- *File Search* (formerly the retrieval plugin): Attach one or more files and they will be RAG-vectorized and available as content to the assistant.\n",
    "- *Code Interpreter*: Run python code in a secure sandbox.\n",
    "\n",
    "The Assistant API productized as OpenAI's `GPTs` product. The `GPT Builder` lets developers create and deploy GPTs assistants using a web interface.\n",
    "\n",
    "### Chat Driver\n",
    "\n",
    "But an \"assistant\" requires pretty strong \"abstraction lock-in\". This thing isn't really an assistant in the fullest sense... it's more like a \"pseudo-assistant\", but this confuses things. Let's just let the Chat Completion API be what it is and drive it as necessary as we create our assistants. Let's just wrap up the function calling bits (which, ultimately, can give you the other tools like Functions and File Search) in a simple-to-use GPT-like interface we'll call a *chat driver*.\n",
    "\n",
    "The chat driver is meant to be used the exact way the Chat Completions API is... just easier.\n",
    "\n",
    "Our chat driver provides:\n",
    "\n",
    "- The ability to almost magically register functions to the function tool using a `FunctionRegistry`.\n",
    "- Tracking of message history.\n",
    "- Management of a `Context` object that can be used for session management and supply additional context to functions.\n",
    "- Some prompt creation helpers.\n",
    "- Other utilities... this is just meant to be an interface you can use to forget about all the api complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_driver import ChatDriver, ChatDriverConfig, Context\n",
    "from typing import List\n",
    "from openai.types.chat import ChatCompletionMessageParam\n",
    "\n",
    "\n",
    "# When an chat driver is created, it will automatically create a context with a\n",
    "# session_id. Or, if you want to use a specific session_id, you can pass it as\n",
    "# an argument. This is useful for scoping this chat driver instance to an\n",
    "# external identifier.\n",
    "context = Context(\"conversation-id-1000\")\n",
    "\n",
    "\n",
    "# Define tool functions for the chat driver. All functions used by the chat driver\n",
    "# require a session_id as the first argument.\n",
    "def get_file_contents(context: Context, file_path: str) -> str:\n",
    "    \"\"\"Return the contents of a file.\"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(context: Context, name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{context.session_id}: {name} erased\"\n",
    "\n",
    "\n",
    "# Define the chat driver.\n",
    "instructions = \"You are a helpful assistant.\"\n",
    "\n",
    "# Define the conversation so far.\n",
    "messages: List[ChatCompletionMessageParam] = []\n",
    "\n",
    "chat_driver = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        messages=messages,\n",
    "        context=context,\n",
    "        commands=[erase],  # Commands can be registered when instantiating the chat driver.\n",
    "        functions=[erase],  # Functions can be registered when instantiating the chat driver.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Let's clear the data from previous runs.\n",
    "chat_driver.clear_session_data()\n",
    "\n",
    "\n",
    "# You can also use the `register_function` decorator to register a function.\n",
    "# Remember, all functions used by the chat driver require a session_id as the\n",
    "# first argument.\n",
    "@chat_driver.register_function_and_command\n",
    "def echo(context: Context, text: str) -> str:\n",
    "    \"\"\"Return the text.\"\"\"\n",
    "    return f\"{context.session_id}: {text}\"\n",
    "\n",
    "\n",
    "# You can also register functions manually.\n",
    "chat_driver.register_function_and_command(get_file_contents)\n",
    "\n",
    "# Ok. Let's see if we got one.\n",
    "print(chat_driver.context.session_id)\n",
    "\n",
    "# Let's see if the agent can respond.\n",
    "response = await chat_driver.respond(\"Hi, my name is Paul.\")\n",
    "print(response.message)\n",
    "\n",
    "# Help command (shows command available).\n",
    "response = await chat_driver.respond(\"/help\")\n",
    "print(response.message)\n",
    "\n",
    "# We can run any function or command directly.\n",
    "response = await chat_driver.functions.echo(\"Hi, my name is Paul.\")\n",
    "print(response)\n",
    "\n",
    "# Let's see if the chat driver has the ability to run it's own registered function.\n",
    "response = await chat_driver.respond(\"Please tell me what's in file 123.txt.\")\n",
    "print(response.message)\n",
    "\n",
    "# Let's see the full response event.\n",
    "print(response.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with a chat driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from chat_driver import ChatDriverConfig, ChatDriver\n",
    "from context import Context\n",
    "\n",
    "context = Context(\"conversation-id-1001\")\n",
    "\n",
    "\n",
    "def get_file_contents(context: Context, file_path: str) -> str:\n",
    "    \"\"\"Returns the contents of a file.\"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(context: Context, name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{context.session_id}: {name} erased\"\n",
    "\n",
    "\n",
    "def echo(context: Context, value: Any) -> str:\n",
    "    \"\"\"Echos a value as a string.\"\"\"\n",
    "    match value:\n",
    "        case str():\n",
    "            return value\n",
    "        case list():\n",
    "            return \", \".join(map(str, value))\n",
    "        case dict():\n",
    "            return json.dumps(value)\n",
    "        case int() | bool() | float():\n",
    "            return str(value)\n",
    "        case _:\n",
    "            return str(value)\n",
    "\n",
    "\n",
    "functions = [get_file_contents, erase, echo]\n",
    "\n",
    "# Define the chat driver.\n",
    "chat_driver_config = ChatDriverConfig(\n",
    "    openai_client=async_client,\n",
    "    model=model,\n",
    "    instructions=\"You are an assistant that has access to a sand-boxed Posix shell.\",\n",
    "    context=context,\n",
    "    commands=functions,\n",
    "    functions=functions,\n",
    ")\n",
    "\n",
    "chat_driver = ChatDriver(chat_driver_config)\n",
    "\n",
    "# Note: Look in the .data directory for the logs, message history, and other data.\n",
    "\n",
    "# Chat with the skill.\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    response = await chat_driver.respond(message)\n",
    "    # You can print the entire response event! \n",
    "    # print(response.to_json())\n",
    "    print(f\"Assistant: {response.message}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous\n",
    "\n",
    "Chaining together chat drivers in more interesting ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A router \"mini-gpt\" assistant\n",
    "\n",
    "(this is just an idea I was trying out... it doesn't work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_driver import ChatDriver, ChatDriverConfig, Context\n",
    "from enum import Enum\n",
    "from events import BaseEvent\n",
    "\n",
    "\n",
    "class InteractionMode(Enum):\n",
    "    \"\"\"The possible interaction modes of the assistant.\"\"\"\n",
    "\n",
    "    INTRO = \"intro\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "    BRAINSTORMING = \"brainstorming\"\n",
    "\n",
    "\n",
    "context = Context(\"conversation-id-123\")\n",
    "session_state = {}\n",
    "session_state[context.session_id] = {\"interaction_mode\": InteractionMode.INTRO}\n",
    "\n",
    "\n",
    "# Define the intro assistant.\n",
    "intro_instructions = (\n",
    "    \"You are a introduction assistant that gathers a user's name and a topic they would \"\n",
    "    \"like to discuss. \\n\\n\"\n",
    "    'Once you have this information, respond with DONE: { \"name\": string, \"topic\": string }.'\n",
    ")\n",
    "intro_assistant = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=intro_instructions,\n",
    "        context=context,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the brainstorming assistant.\n",
    "brainstorming_instructions = (\n",
    "    \"You are a brainstorming assistant that generates creative responses. You can generate \"\n",
    "    \"ideas, concepts, or suggestions on the user's topic. Once the user indicates they are done \"\n",
    "    \"brainstorming, respond with DONE: {}.\"\n",
    ")\n",
    "brainstorming_assistant = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=brainstorming_instructions,\n",
    "        context=context,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the conversational assistant.\n",
    "conversational_instructions = \"You are a conversational assistant that generates engaging responses.\"\n",
    "conversational_assistant = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=conversational_instructions,\n",
    "        context=context,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the modal assistant.\n",
    "instructions = (\n",
    "    \"You are a message routing assistant. While interacting with the user, over a long conversation, \"\n",
    "    'you may decide to enter into new \"interaction mode\". An interaction mode is a state in which '\n",
    "    \"the assistant behaves differently. For example, in a brainstorming mode, the assistant may generate \"\n",
    "    \"more creative responses. In a conversational mode, the assistant may generate more engaging responses. \"\n",
    "    \"In an intro mode, the assistant may generate more informative responses. You can switch between \"\n",
    "    \"these modes by calling the set_interaction_mode function. Always forward the user's message to the \"\n",
    "    \"appropriate assistant based on the current interaction mode.\\n\\n\"\n",
    "    \"Always start a conversation by switching to the intro mode.\"\n",
    "    \"<CURRENT_INTERACTION_MODE>{interaction_mode}</CURRENT_INTERACTION_MODE>\"\n",
    ")\n",
    "\n",
    "\n",
    "def set_interaction_mode(session_id: str, mode: InteractionMode) -> None:\n",
    "    \"\"\"Set the assistant to a specific interaction mode.\"\"\"\n",
    "    print(f\"Setting interaction mode to: {mode}\")\n",
    "    session_state[session_id][\"interaction_mode\"] = mode\n",
    "\n",
    "\n",
    "def get_interaction_mode(session_id: str) -> InteractionMode:\n",
    "    \"\"\"Get the current interaction mode of the assistant.\"\"\"\n",
    "    return session_state[session_id][\"interaction_mode\"]\n",
    "\n",
    "\n",
    "async def forward_message(session_id: str, interaction_mode: InteractionMode, message: str) -> BaseEvent:\n",
    "    \"\"\"Forward a message to the appropriate assistant based on the current interaction mode.\"\"\"\n",
    "    print(f\"Forwarding message to assistant in mode: {interaction_mode}\")\n",
    "    if interaction_mode == InteractionMode.INTRO:\n",
    "        response = await intro_assistant.respond(message)\n",
    "    elif interaction_mode == InteractionMode.BRAINSTORMING:\n",
    "        response = await brainstorming_assistant.respond(message)\n",
    "    else:\n",
    "        response = await conversational_assistant.respond(message)\n",
    "    print(f\"Assistant response from forward_message{interaction_mode}: {response}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "modal_assistant = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=\"gpt-4o\",\n",
    "        instructions=instructions,\n",
    "        context=context,\n",
    "        functions=[set_interaction_mode, forward_message],\n",
    "    )\n",
    ")\n",
    "modal_assistant.functions.set_interaction_mode(InteractionMode.INTRO)\n",
    "\n",
    "# Begin the conversation.\n",
    "\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    response = modal_assistant.respond(\n",
    "        message, instruction_parameters={\"interaction_mode\": get_interaction_mode(context.session_id)}\n",
    "    )\n",
    "    print(f\"Modal Assistant: {response}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
