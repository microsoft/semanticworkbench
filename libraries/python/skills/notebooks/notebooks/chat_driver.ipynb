{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Driver\n",
    "\n",
    "An OpenAI Chat Completions API wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "Run this cell to set the notebook up. Other sections can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import aio, DefaultAzureCredential, get_bearer_token_provider, AzureCliCredential\n",
    "\n",
    "from openai import AsyncAzureOpenAI, AzureOpenAI\n",
    "\n",
    "import logging \n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOGGING = {\n",
    "    \"version\": 1,\n",
    "    \"disable_existing_loggers\": False,\n",
    "    \"formatters\": {\n",
    "        \"json\": {\n",
    "            \"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\",\n",
    "            \"fmt\": \"%(asctime)s %(levelname)s %(name)s %(message)s\",\n",
    "\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Set up structured logging to a file. All of the cells in this notebook use\n",
    "# this logger. Find them at .data/logs.jsonl.\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record) -> str:\n",
    "        record_dict = record.__dict__\n",
    "        log_record = {\n",
    "            'timestamp': self.formatTime(record, self.datefmt),\n",
    "            'level': record.levelname,\n",
    "            'session_id': record_dict.get('session_id', None),\n",
    "            'run_id': record_dict.get('run_id', None),\n",
    "            'message': record.getMessage(),\n",
    "            'data': record_dict.get('data', None),\n",
    "            'module': record.module,\n",
    "            'funcName': record.funcName,\n",
    "            'lineNumber': record.lineno,\n",
    "            'logger': record.name,\n",
    "        }\n",
    "        extra_fields = {\n",
    "            key: value for key, value in record.__dict__.items() \n",
    "            if key not in ['levelname', 'msg', 'args', 'exc_info', 'funcName', 'module', 'lineno', 'name', 'message', 'asctime', 'session_id', 'run_id', 'data']\n",
    "        }\n",
    "        log_record.update(extra_fields)\n",
    "        return json.dumps(log_record)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "modules = ['httpcore.connection', 'httpcore.http11', 'httpcore.sync.connection', 'httpx', 'openai', 'urllib3.connectionpool', 'urllib3.util.retry']\n",
    "for module in modules:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "data_dir = Path('.data')\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "handler = logging.FileHandler(data_dir / 'logs.jsonl')\n",
    "handler.setFormatter(JsonFormatter())\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "azure_openai_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "    \"max_retries\": 2,\n",
    "}\n",
    "logger.info(\"Azure OpenAI configuration\", extra=azure_openai_config)\n",
    "\n",
    "async_client = AsyncAzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=aio.get_bearer_token_provider(\n",
    "        aio.AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    **azure_openai_config,\n",
    "    azure_ad_token_provider=get_bearer_token_provider(\n",
    "        AzureCliCredential(),\n",
    "        \"https://cognitiveservices.azure.com/.default\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "model: str = azure_openai_config.get(\"azure_deployment\", \"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatCompletionsAPI usage\n",
    "\n",
    "Azure/OpenAI's Chat Completions API is the fundamental building block of an AI assistant that uses the GPT model. \n",
    "\n",
    "- https://platform.openai.com/docs/api-reference/chat\n",
    "- https://github.com/openai/openai-python/blob/main/api.md\n",
    "- https://platform.openai.com/docs/api-reference/chat drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-ARRAZhqMa0AhQEzs0YAstZfa5CMm8\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"This is a test. How can I assist you further?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1731102659,\n",
      "  \"model\": \"gpt-4o-2024-08-06\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": \"fp_d54531d9eb\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 12,\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 24,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AQgaCid05i8dGgUHijQsvPAsMYbv4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1730923580, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_d54531d9eb', usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "message_event = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "print(message_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': '', 'function_call': None, 'refusal': None, 'role': 'assistant', 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': 'This', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': ' is', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': ' a', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': ' test', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': '.', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n",
      "{'id': 'chatcmpl-AQgaEhy4ed19h72VpBu8nJBJE3g3L', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1730923582, 'model': 'gpt-4o-2024-08-06', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_d54531d9eb', 'usage': None}\n"
     ]
    }
   ],
   "source": [
    "stream = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    stream=True,\n",
    ")\n",
    "async for chunk in stream:\n",
    "    print(chunk.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized response handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is both exciting and complex, with its trajectory shaped by advances in technology, ethical considerations, and societal needs. As we look ahead, several key themes emerge:\n",
      "\n",
      "1. **Integration and Personalization**: AI will become increasingly integrated into our daily lives, driving personalized experiences. From healthcare to education, AI systems will tailor recommendations and interventions to individual needs, optimizing outcomes across various sectors.\n",
      "\n",
      "2. **AI in Healthcare**: We can expect AI to revolutionize the healthcare industry by enhancing diagnostic accuracy, streamlining administrative processes, and developing personalized medicine. AI-driven tools could predict diseases, recommend treatments, and even aid in surgical procedures, ultimately improving patient care and reducing costs.\n",
      "\n",
      "3. **Autonomous Systems**: Autonomous vehicles, drones, and robotic systems will become more prevalent, transforming industries like transportation, logistics, and manufacturing. These systems will improve efficiency and safety while also creating new business models and opportunities.\n",
      "\n",
      "4. **Ethical and Responsible AI**: As AI systems become more autonomous, the demand for ethical AI frameworks will increase. Ensuring transparency, fairness, accountability, and privacy will be crucial to maintaining public trust. Developing AI systems that align with human values and societal norms will be an ongoing challenge.\n",
      "\n",
      "5. **AI and the Workforce**: While AI will automate certain tasks, it will also create new job opportunities and demand an upskilled workforce. It is important to focus on reskilling and education initiatives to prepare the workforce for an AI-augmented future.\n",
      "\n",
      "6. **AI in Climate and Sustainability**: AI will play a significant role in addressing climate change and promoting sustainability. From optimizing energy usage and improving agricultural practices to predicting environmental changes, AI could be a critical tool in creating a more sustainable future.\n",
      "\n",
      "7. **AI in Creativity and Arts**: AI is increasingly being used in creative fields, such as music, art, and literature. While it can augment human creativity by offering new tools and perspectives, there will be ongoing discussions about the nature of creativity and authorship.\n",
      "\n",
      "8. **AI Governance and Policy**: With AI's growing influence, policy and regulation will need to keep pace to address issues such as data ownership, algorithmic bias, and national security. International collaboration may be necessary to create coherent frameworks addressing these challenges.\n",
      "\n",
      "In summary, AI has the potential to greatly benefit society, but its development requires thoughtful consideration of ethical, social, and economic impacts. As we move forward, collaboration among technologists, policymakers, and the public will be essential to harness AI for the greater good.\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.logging import make_completion_args_serializable, add_serializable_data\n",
    "from openai_client.completion import message_content_from_completion\n",
    "\n",
    "context = Context(\"conversation-id-1005\")\n",
    "\n",
    "# We use a metadata dictionary in our helpers to store information about the\n",
    "# completion request.\n",
    "metadata = {}\n",
    "\n",
    "# This is just standard OpenAI completion request arguments.\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and answer thoughtfully.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# If we these completion args to logs and metadata, though, they need to be\n",
    "# serializable. We have a helper for that.\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "\n",
    "# We have helpers for validating the response and handling exceptions in a\n",
    "# standardized way. These ensure that logging happens and metadata is loaded up\n",
    "# properly.\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "\n",
    "    # This helper looks for any error-like situations (the model refuses to\n",
    "    # answer, content filters, incomplete responses) and throws exceptions that\n",
    "    # are handled by the next helper. The first argument is an identifier that\n",
    "    # will be used for logs and metadata namespacing.\n",
    "    validate_completion(completion)\n",
    "    logger.debug(\"completion response.\", extra=add_serializable_data(completion))\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "\n",
    "except Exception as e:\n",
    "    # This helper processes all the types of error conditions you might get from\n",
    "    # the OpenAI API in a standardized way.\n",
    "    completion_error = CompletionError(e)\n",
    "    print(completion_error)\n",
    "    print(completion_error.body)\n",
    "\n",
    "else:\n",
    "    # The message_string helper is used to extract the response from the completion\n",
    "    # (which can get tedious).\n",
    "    print(message_content_from_completion(completion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"AI is rapidly evolving and has the potential to transform virtually every industry. With advancements in machine learning, natural language processing, and robotics, AI will continue to play a crucial role in automating routine tasks and providing intelligent insights.\",\n",
      "  \"answer\": \"The future of AI involves deeper integration into everyday life and industry. We will likely see AI systems becoming more autonomous, sophisticated, and seamlessly integrated into systems like healthcare, transportation, and personalized services. AI's future encompasses not only technological advancements but also ethical considerations and ensuring beneficial societal impacts.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "\n",
    "from openai_client.logging import make_completion_args_serializable, add_serializable_data\n",
    "from openai_client.completion import message_content_dict_from_completion, JSON_OBJECT_RESPONSE_FORMAT\n",
    "\n",
    "context = Context(\"conversation-id-1002\")\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and return your answer as valid JSON like { \\\"thoughts\\\": <some thoughts>, \\\"answer\\\": <an answer> }.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": JSON_OBJECT_RESPONSE_FORMAT,\n",
    "}\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "    validate_completion(completion)\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    logger.error(completion_error.message, extra=add_serializable_data({\"error\": completion_error.body, \"metadata\": metadata}))\n",
    "else:\n",
    "    message = message_content_dict_from_completion(completion)\n",
    "    print(json.dumps(message, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured output\n",
    "\n",
    "Any Pydantic BaseModel can be used as the \"response_format\" and OpenAI will try to load it up for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"The future of AI is a convergence of technological advancement, ethical consideration, and societal adaptation. Its trajectory will be shaped by how well we manage its integration into daily life, ensuring it remains a tool to augment human capabilities rather than replace them entirely.\",\n",
      "  \"answer\": \"The future of AI is incredibly promising and multifaceted, with potential developments across various domains. In healthcare, AI may revolutionize diagnostics and personalized medicine, allowing for earlier detection of diseases and tailored treatment plans. In the realm of transportation, we can expect more sophisticated autonomous vehicles, improving safety and efficiency on our roads. Moreover, AI could enhance environmental monitoring and response, aiding in the fight against climate change by optimizing energy use and predicting environmental changes.\\n\\nHowever, as we look to this future, it is crucial to address the ethical implications of AI. This includes ensuring data privacy, reducing algorithmic bias, and contemplating the social impacts of automation on employment. Governance and regulation will play vital roles in guiding AI development to benefit society as a whole, promoting fairness, transparency, and accountability.\\n\\nUltimately, the future of AI will be defined by how humans choose to steer its development, balancing innovation with the responsibilities it entails. By fostering collaboration among technologists, ethicists, lawmakers, and the general public, we can harness AI's potential to create a more informed, just, and equitable world.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from context import Context\n",
    "from pydantic import BaseModel\n",
    "from typing import cast\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.logging import add_serializable_data, make_completion_args_serializable\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    thoughts: str\n",
    "    answer: str\n",
    "\n",
    "context = Context(\"conversation-id-1002\")\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members and return your thoughtful answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the future of AI?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": Output,\n",
    "}\n",
    "\n",
    "metadata[\"completion_args\"] = make_completion_args_serializable(completion_args)\n",
    "try:\n",
    "    completion = await async_client.beta.chat.completions.parse(**completion_args)\n",
    "    validate_completion(completion)\n",
    "    metadata[\"completion\"] = completion.model_dump()\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    logger.error(completion_error.message, extra=add_serializable_data({\"error\": completion_error.body, \"metadata\": metadata}))\n",
    "else:\n",
    "    # The parsed message is in the `parsed` attribute.\n",
    "    output = cast(Output, completion.choices[0].message.parsed)\n",
    "    print(output.model_dump_json(indent=2))\n",
    "\n",
    "    # Or you can just get the text of the message like usual.\n",
    "    # print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple tool usage\n",
    "\n",
    "The OpenAI chat completions API used the idea of \"tools\" to let the model request running a local tool and then processing the output. To use it, you need to create a JSON Schema representation of the function you want to use as a tool, check the response for the model requesting to run that function, run the function, and give the model the results of the function run for a final call.\n",
    "\n",
    "While you can continue doing all of this yourself, our `complete_with_tool_calls` helper function makes this all easier for you.\n",
    "\n",
    "Instead of generating JSON schema and executing functions yourself, you can use our `ToolFunctions` class to define the functions you want to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square of 53 is 2809.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "def square_the_number(number: int) -> int:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    return number * number\n",
    "\n",
    "\n",
    "tool_functions = ToolFunctions([\n",
    "    ToolFunction(square_the_number),\n",
    "])\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the square of 53?\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, tool_functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        print(completion.choices[0].message.content)\n",
    "        # print(json.dumps(metadata, indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured tool inputs and output\n",
    "\n",
    "You can use Pydantic models as input to tool function arguments.\n",
    "\n",
    "You can also tell the model to respond with structured JSON or Pydantic model structures.\n",
    "\n",
    "Here's an example of doing both things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thoughts\": \"The weather is typically sunny in Beverly Hills, especially considering the geographic location known for its warm climate.\",\n",
      "  \"answer\": \"The weather in the 90210 area (Beverly Hills) is currently sunny with a temperature of 25°C (77°F). There is minimal cloud cover, at 20%.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import cast\n",
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    zipcode: str\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    description: str = Field(description=\"The weather description.\")\n",
    "    cloud_cover: float\n",
    "    temp_c: float\n",
    "    temp_f: float\n",
    "\n",
    "def get_weather(input: Input) -> Weather:\n",
    "    \"\"\"Return the weather.\"\"\"\n",
    "    return Weather(description=\"Sunny\", cloud_cover=0.2, temp_c=25.0, temp_f=77.0)\n",
    "\n",
    "class Output(BaseModel):\n",
    "    thoughts: str\n",
    "    answer: str\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is the weather in 90210?\",\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": Output,\n",
    "}\n",
    "\n",
    "functions = ToolFunctions([\n",
    "    ToolFunction(get_weather),\n",
    "])\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        # The parsed message is in the `parsed` attribute.\n",
    "        output = cast(Output, completion.choices[0].message.parsed)\n",
    "        print(output.model_dump_json(indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n",
    "\n",
    "    # Or you can just get the text of the message like usual.\n",
    "    # print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool functions with shadowed locals\n",
    "\n",
    "If you want to make a local function available to be run as a chat completion\n",
    "tool, you can. However, oftentimes, the local function might have some extra\n",
    "arguments that you don't want the model to have to fill out for you in a tool\n",
    "call. In this case, you can create a wrapper function that has the same\n",
    "signature as the tool call and then calls the local function with the extra\n",
    "arguments filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the fish calculation on the number 53 is: `0b101011111001` in binary representation.\n"
     ]
    }
   ],
   "source": [
    "from openai_client.errors import CompletionError, validate_completion\n",
    "from openai_client.tools import complete_with_tool_calls, ToolFunctions, ToolFunction\n",
    "\n",
    "# Here is the real function that does the work.\n",
    "def real_square_the_number(number: int, binary: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        return bin(number * number)\n",
    "    return str(number * number)\n",
    "\n",
    "\n",
    "# Here is the wrapper function that whose signature will be used as the tool\n",
    "# call. You can just have it calls the real function with the extra arguments\n",
    "# filled in.\n",
    "def fish_calc(number: int) -> str:\n",
    "    \"\"\"\n",
    "    Return the square of the number.\n",
    "    \"\"\"\n",
    "    return real_square_the_number(number, binary=True)\n",
    "\n",
    "# Add then just add wrapper to the tool functions you pass to the\n",
    "# `complete_with_tool_calls` function. This is a way you can expose _any_\n",
    "# function to be called by the model, but with the args you want the model to\n",
    "# fill in!\n",
    "tool_functions = ToolFunctions([\n",
    "    ToolFunction(fish_calc),\n",
    "])\n",
    "\n",
    "metadata = {}\n",
    "completion_args = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Run a fish calculation on 53 for me.\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    completion, new_messages = await complete_with_tool_calls(async_client, completion_args, tool_functions, metadata)\n",
    "    validate_completion(completion)\n",
    "except Exception as e:\n",
    "    completion_error = CompletionError(e)\n",
    "    metadata[\"completion_error\"] = completion_error.body\n",
    "    print(completion_error.message)\n",
    "    print(completion_error.body)\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    if completion:\n",
    "        print(completion.choices[0].message.content)\n",
    "        # print(json.dumps(metadata, indent=2))\n",
    "    else:\n",
    "        print(\"No completion returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion Driver (a.k.a \"chat driver\")\n",
    "\n",
    "### OpenAI Assistants\n",
    "\n",
    "The Azure/OpenAI Assistants API is newer, stateful API that splits an `assistant` from the data about a conversation `thread` that can be `run` against an `assistant`. Additionally, you can add `tools` to an assistant that enable the assistant to have more interactive capabilities. The tools currently available are:\n",
    "\n",
    "- *Functions*: Registering local functions with the assistant so it knows it can call them before generating a response. This is a \"hold on let me look that up for you\" kind of interaction.\n",
    "- *File Search* (formerly the retrieval plugin): Attach one or more files and they will be RAG-vectorized and available as content to the assistant.\n",
    "- *Code Interpreter*: Run python code in a secure sandbox.\n",
    "\n",
    "The Assistant API productized as OpenAI's `GPTs` product. The `GPT Builder` lets developers create and deploy GPTs assistants using a web interface.\n",
    "\n",
    "### Chat Driver\n",
    "\n",
    "But an \"assistant\" requires pretty strong \"abstraction lock-in\". This thing isn't really an assistant in the fullest sense... it's more like a \"pseudo-assistant\", but this confuses things. Let's just let the Chat Completion API be what it is and drive it as necessary as we create our assistants. Let's just wrap up the function calling bits (which, ultimately, can give you the other tools like Functions and File Search) in a simple-to-use GPT-like interface we'll call a *chat driver*.\n",
    "\n",
    "The chat driver is meant to be used the exact way the Chat Completions API is... just easier.\n",
    "\n",
    "Our chat driver provides:\n",
    "\n",
    "- The ability to almost magically register functions to the function tool using a `FunctionRegistry`.\n",
    "- Tracking of message history.\n",
    "- Management of a `Context` object that can be used for session management and supply additional context to functions.\n",
    "- Some prompt creation helpers.\n",
    "- Other utilities... this is just meant to be an interface you can use to forget about all the api complexities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the simplest usage of a chat driver\n",
    "\n",
    "Notice that a .data directory is created by default. This is where the conversation history is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is both exciting and complex, with the potential to profoundly transform many aspects of our lives over the coming decades. Let's explore a few key areas where AI is expected to make significant impacts:\n",
      "\n",
      "1. **Healthcare**: AI has the potential to revolutionize healthcare by improving diagnostics, personalizing treatment plans, and enhancing patient monitoring. Machine learning algorithms can analyze vast amounts of medical data to detect patterns that might be missed by human doctors, leading to earlier and more accurate diagnoses.\n",
      "\n",
      "2. **Autonomous Systems**: From self-driving cars to drones, autonomous systems powered by AI are likely to become increasingly common. These systems can improve efficiency, safety, and accessibility in transportation and logistics, while also opening up new possibilities in areas like agriculture and disaster response.\n",
      "\n",
      "3. **Work and Productivity**: AI can automate routine tasks, freeing up humans to focus on more complex and creative work. This has the potential to increase productivity and innovation, but also raises questions about job displacement and the future of work, necessitating thoughtful discussions on workforce retraining and the evolution of job roles.\n",
      "\n",
      "4. **Personalization**: AI can enhance consumer experiences by providing personalized recommendations and services, from content and product suggestions to educational platforms tailored to individual learning styles. This can lead to increased satisfaction and efficiency, but also raises issues regarding privacy and data security.\n",
      "\n",
      "5. **Scientific Research**: AI is becoming an indispensable tool in scientific research, capable of analyzing complex datasets more quickly and accurately than traditional methods. This can accelerate discoveries in fields such as genomics, climate modeling, and materials science, potentially leading to groundbreaking innovations.\n",
      "\n",
      "6. **Ethics and Fairness**: As AI becomes more integrated into society, it's crucial to address ethical considerations, including biases in AI systems and the impact on privacy and decision-making processes. Developing fair, transparent, and accountable AI systems will be essential for gaining public trust and maximizing benefits.\n",
      "\n",
      "7. **Global Challenges**: AI could play a crucial role in addressing global challenges like climate change, resource management, and sustainable development. By optimizing processes and offering innovative solutions, AI can contribute to making societies more resilient and adaptable.\n",
      "\n",
      "Overall, the future of AI promises to be a catalyst for growth and change across various domains. However, realizing this potential will require collaborative efforts from researchers, policymakers, businesses, and the public to ensure these technologies are developed and deployed in ways that are ethical and beneficial to all of humanity.\n"
     ]
    }
   ],
   "source": [
    "from chat_driver import ChatDriver, ChatDriverConfig\n",
    "\n",
    "instructions = \"You are a famous computer scientist. You are giving a talk at a conference. You are talking about the future of AI and how it will change the world. You are asked a questions by audience members.\"\n",
    "\n",
    "chat_driver = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "    ),\n",
    ")\n",
    "\n",
    "message_event = await chat_driver.respond(\"What is the future of AI?\")\n",
    "print(message_event.message)\n",
    "# print(message_event.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can register functions to chat drivers\n",
    "\n",
    "Chat drivers will use any functions you give it as both OpenAI tool calls, and as commands.\n",
    "\n",
    "With each response call, you can specify what type of response you want to have... string, dictionary, or Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, Paul! How can I assist you today?\n",
      "\n",
      "Commands:\n",
      "echo(text: str): Return the text.\n",
      "erase(name: str): Erases a stored value.\n",
      "get_file_contents(file_path: str): Return the contents of a file.\n",
      "\n",
      "Args:\n",
      "- file_path: The path to the file.\n",
      "get_weather(input: Input): Return the weather.\n",
      "help(): Return this help message.\n",
      "json_thing(): Return json.\n",
      "\n",
      "Echoing: Echo this.\n",
      "\n",
      "The content of \"123.txt\" is: \"The purpose of life is to be happy.\" How else can I help you today?\n",
      "\n",
      "{\"description\":\"Sunny\",\"cloud_cover\":0.2,\"temp_c\":25.0,\"temp_f\":77.0}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, cast\n",
    "from chat_driver import ChatDriver, ChatDriverConfig\n",
    "from chat_driver import LocalMessageHistoryProvider\n",
    "from pydantic import BaseModel, Field\n",
    "from openai_client.tools import ToolFunctions, ToolFunction\n",
    "\n",
    "\n",
    "# When an chat driver is created, it will automatically create a context with a\n",
    "# session_id. Or, if you want to use a specific session_id, you can pass it as\n",
    "# an argument. This is useful for scoping this chat driver instance to an\n",
    "# external identifier.\n",
    "session_id = \"conversation-id-1002\"\n",
    "\n",
    "\n",
    "# Define tool functions for the chat driver.\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the contents of a file.\n",
    "\n",
    "    Args:\n",
    "    - file_path: The path to the file.\n",
    "    \"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{context.session_id}: {name} erased\"\n",
    "\n",
    "def json_thing() -> dict[str, Any]:\n",
    "    \"\"\"Return json.\"\"\"\n",
    "    return {\"key\": \"value\"}\n",
    "\n",
    "class Input(BaseModel):\n",
    "    zipcode: str\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    description: str = Field(description=\"The weather description.\")\n",
    "    cloud_cover: float\n",
    "    temp_c: float\n",
    "    temp_f: float\n",
    "\n",
    "def get_weather(input: Input) -> Weather:\n",
    "    \"\"\"Return the weather.\"\"\"\n",
    "    return Weather(description=\"Sunny\", cloud_cover=0.2, temp_c=25.0, temp_f=77.0)\n",
    "\n",
    "# Define the chat driver.\n",
    "instructions = \"You are a helpful assistant.\"\n",
    "\n",
    "all_funcs = [ get_file_contents, erase, json_thing, get_weather ]\n",
    "\n",
    "chat_driver = ChatDriver(\n",
    "    ChatDriverConfig(\n",
    "        openai_client=async_client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        # message_provider=message_provider,\n",
    "        commands=all_funcs,  # Commands can be registered when instantiating the chat driver.\n",
    "        functions=all_funcs,  # Functions can be registered when instantiating the chat driver.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Let's clear the data from previous runs by using a custom message provider.\n",
    "message_provider = cast(LocalMessageHistoryProvider, chat_driver.message_provider)\n",
    "message_provider.delete_all()\n",
    "\n",
    "\n",
    "# You can also use the `register_function` decorator to register a function.\n",
    "@chat_driver.register_function_and_command\n",
    "def echo(text: str) -> str:\n",
    "    \"\"\"Return the text.\"\"\"\n",
    "    return f\"Echoing: {text}\"\n",
    "\n",
    "\n",
    "# You can also register functions manually.\n",
    "chat_driver.register_function_and_command(get_file_contents)\n",
    "\n",
    "# Let's see if the agent can respond.\n",
    "message_event = await chat_driver.respond(\"Hi, my name is Paul.\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Help command (shows command available).\n",
    "message_event = await chat_driver.respond(\"/help\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# We can run any function or command directly.\n",
    "message_event = await chat_driver.functions.echo(\"Echo this.\")\n",
    "print()\n",
    "print(message_event)\n",
    "\n",
    "# Let's see if the chat driver has the ability to run it's own registered function.\n",
    "message_event = await chat_driver.respond(\"Please tell me what's in file 123.txt.\")\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Stuctured output.\n",
    "message_event = await chat_driver.respond(\"What is the weather in 90210?\", response_format=Weather)\n",
    "print()\n",
    "print(message_event.message)\n",
    "\n",
    "# Let's see the full response event.\n",
    "# print()\n",
    "# print(response.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with a chat driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi!\n",
      "Assistant: Hello! How can I assist you today?\n",
      "User: What's the capital of America?\n",
      "Assistant: The capital of the United States of America is Washington, D.C.\n",
      "User: No, I meant South America.\n",
      "Assistant: South America is a continent composed of multiple countries, each with its own capital. Could you specify which country's capital you're interested in within South America?\n",
      "User: Mexico.\n",
      "Assistant: Mexico is actually part of North America. The capital of Mexico is Mexico City.\n",
      "User: Brazil.\n",
      "Assistant: The capital of Brazil is Brasília.\n"
     ]
    }
   ],
   "source": [
    "from chat_driver import ChatDriverConfig, ChatDriver\n",
    "from context import Context\n",
    "from openai_client.tools import ToolFunction, ToolFunctions\n",
    "\n",
    "\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Returns the contents of a file.\"\"\"\n",
    "    return \"The purpose of life is to be happy.\"\n",
    "\n",
    "\n",
    "def erase(name: str) -> str:\n",
    "    \"\"\"Erases a stored value.\"\"\"\n",
    "    return f\"{session_id}: {name} erased\"\n",
    "\n",
    "\n",
    "def echo(value: str) -> str:  # noqa: F811\n",
    "    \"\"\"Echos a value as a string.\"\"\"\n",
    "    match value:\n",
    "        case str():\n",
    "            return value\n",
    "        case list():\n",
    "            return \", \".join(map(str, value))\n",
    "        case dict():\n",
    "            return json.dumps(value)\n",
    "        case int() | bool() | float():\n",
    "            return str(value)\n",
    "        case _:\n",
    "            return str(value)\n",
    "\n",
    "# Define the chat driver.\n",
    "chat_driver_config = ChatDriverConfig(\n",
    "    openai_client=async_client,\n",
    "    model=model,\n",
    "    instructions=\"You are an assistant that has access to a sand-boxed Posix shell.\",\n",
    "    commands=[ get_file_contents, erase, echo ],\n",
    "    functions=[ get_file_contents, erase, echo ],\n",
    ")\n",
    "\n",
    "chat_driver = ChatDriver(chat_driver_config)\n",
    "\n",
    "# Note: Look in the .data directory for the logs, message history, and other data.\n",
    "\n",
    "# Chat with the skill.\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    message_event = await chat_driver.respond(message)\n",
    "    if message_event.metadata.get(\"error\"):\n",
    "        print(f\"Error: {message_event.metadata.get('error')}\")\n",
    "        print(message_event.to_json())\n",
    "        continue\n",
    "    # You can print the entire message event! \n",
    "    # print(response.to_json())\n",
    "    print(f\"Assistant: {message_event.message}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Driver with an Assistant Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from typing import Any, BinaryIO\n",
    "from chat_driver import ChatDriverConfig, ChatDriver, ChatDriverConfig\n",
    "from context import Context\n",
    "from assistant_drive import Drive, DriveConfig, IfDriveFileExistsBehavior \n",
    "\n",
    "def get_drive_from_context(context):\n",
    "    return Drive(DriveConfig(root=f\".data/drive/{context.session_id}\"))\n",
    "\n",
    "def write_file_contents(file_path: str, contents: str) -> str:\n",
    "    \"\"\"Writes the contents to a file.\"\"\"\n",
    "    drive = get_drive_from_context(context)\n",
    "    content_bytes: BinaryIO = BytesIO(contents.encode(\"utf-8\"))\n",
    "    drive.write(content_bytes, file_path, if_exists=IfDriveFileExistsBehavior.OVERWRITE)\n",
    "    return f\"{file_path} updated.\"\n",
    "\n",
    "def read_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Returns the contents of a file.\"\"\"\n",
    "    drive = get_drive_from_context(context)\n",
    "    with drive.open_file(file_path) as file:\n",
    "        return file.read().decode(\"utf-8\")\n",
    "\n",
    "functions = [write_file_contents, read_file_contents]\n",
    "\n",
    "# Define the chat driver.\n",
    "chat_driver_config = ChatDriverConfig(\n",
    "    openai_client=async_client,\n",
    "    model=model,\n",
    "    instructions=\"You are an assistant that has access to a sand-boxed Posix shell.\",\n",
    "    commands=functions,\n",
    "    functions=functions,\n",
    ")\n",
    "\n",
    "chat_driver = ChatDriver(chat_driver_config)\n",
    "\n",
    "# Note: Look in the .data directory for the logs, message history, and other data.\n",
    "\n",
    "# Chat with the skill.\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "    if message == \"\":\n",
    "        break\n",
    "    print(f\"User: {message}\", flush=True)\n",
    "    message_event = await chat_driver.respond(message)\n",
    "    # You can print the entire response event! \n",
    "    # print(response.to_json())\n",
    "    print(f\"Assistant: {message_event.message}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
